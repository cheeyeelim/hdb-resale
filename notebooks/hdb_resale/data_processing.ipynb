{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from hydra import compose, initialize\n",
    "\n",
    "from hdb_resale import sql, utils\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve environment variables\n",
    "load_dotenv()\n",
    "\n",
    "BING_MAP_API_KEY = os.environ.get(\"BING_MAP_API_KEY\")\n",
    "POSTGRESQL_DASH_USER = os.environ.get(\"POSTGRESQL_DASH_USER\")\n",
    "POSTGRESQL_DASH_PASSWORD = os.environ.get(\"POSTGRESQL_DASH_PASSWORD\")\n",
    "POSTGRESQL_DASH_DATABASE = os.environ.get(\"POSTGRESQL_DASH_DATABASE\")\n",
    "POSTGRESQL_HOST = os.environ.get(\"POSTGRESQL_HOST\")\n",
    "POSTGRESQL_PORT = os.environ.get(\"POSTGRESQL_PORT\")\n",
    "\n",
    "# Setup logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create the connection string\n",
    "connection_string = f\"postgresql+psycopg2://{POSTGRESQL_DASH_USER}:{POSTGRESQL_DASH_PASSWORD}@{POSTGRESQL_HOST}:{POSTGRESQL_PORT}/{POSTGRESQL_DASH_DATABASE}\"\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=\"../../airflow_prd/dags/conf\"):\n",
    "    cfg = compose(config_name=\"hdb_resale_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine, metadata = sql.setup_database(\n",
    "    postgresql_dash_user=POSTGRESQL_DASH_USER, \n",
    "    postgresql_dash_password=POSTGRESQL_DASH_PASSWORD, \n",
    "    postgresql_dash_database=POSTGRESQL_DASH_DATABASE,\n",
    "    postgresql_host=POSTGRESQL_HOST,\n",
    "    postgresql_port=POSTGRESQL_PORT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One off data table migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migrate town data\n",
    "\n",
    "town_query = sqlalchemy.text(\"\"\"\n",
    "    SELECT \n",
    "        town, census_mapping, total_population, \n",
    "        population_below24, population_2559, population_above60, \n",
    "        num_hdb, num_condo, num_landed, num_others, \n",
    "        income_below4k, income_4k8k, income_8k12k, income_above12k, \n",
    "        household_wodisability, household_withdisability, \n",
    "        pct_population_below24, pct_population_2559, pct_population_above60, \n",
    "        pct_hdb, pct_condo, pct_landed, \n",
    "        pct_income_below4k, pct_income_4k8k, pct_income_8k12k, pct_income_above12k,\n",
    "        pct_household_wodisability, pct_household_withdisability, \n",
    "        lat, lng \n",
    "    FROM hdb_resale.town\n",
    "\"\"\")\n",
    "town_data = pd.read_sql(town_query, con=engine)\n",
    "\n",
    "town_data[\"source_name\"] = \"singstat_census_2020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create row hash identifier using key columns\n",
    "town_data[\"_row_hash_id\"] = town_data[cfg.database.town_data.hash_key_columns].apply(utils.create_hash, axis=1)\n",
    "\n",
    "# Remove data based on primary key\n",
    "# Needs to be done before data insertion to prevent database duplicated errors\n",
    "# No data will be removed if the primary key does not exist\n",
    "sql.delete_data_primary_key(\n",
    "    engine=engine,\n",
    "    metadata=metadata,\n",
    "    schema_table_name=cfg.database.town_data.schema_table_name,\n",
    "    primary_key=town_data[\"_row_hash_id\"].to_list(),\n",
    ")\n",
    "\n",
    "# Insert data into database\n",
    "with engine.connect() as con:\n",
    "    town_data.to_sql(\n",
    "        name=cfg.database.town_data.table_name, schema=cfg.database.town_data.schema_name, con=con, if_exists=\"append\", index=False, chunksize=10000\n",
    "    )\n",
    "\n",
    "logger.info(f\"Loaded {town_data.shape[0]} rows of data into table {cfg.database.town_data.schema_table_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migrate address data\n",
    "\n",
    "address_query = sqlalchemy.text(\"\"\"\n",
    "    SELECT \n",
    "        block, street_name, town, \n",
    "        country, full_address, \n",
    "        lat, lng \n",
    "    FROM hdb_resale.geolocation\n",
    "\"\"\")\n",
    "address_data = pd.read_sql(address_query, con=engine)\n",
    "\n",
    "address_data[\"source_name\"] = \"bing_map_api\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create row hash identifier using key columns\n",
    "address_data[\"_row_hash_id\"] = address_data[cfg.database.address_data.hash_key_columns].apply(utils.create_hash, axis=1)\n",
    "\n",
    "# Remove data based on primary key\n",
    "# Needs to be done before data insertion to prevent database duplicated errors\n",
    "# No data will be removed if the primary key does not exist\n",
    "sql.delete_data_primary_key(\n",
    "    engine=engine,\n",
    "    metadata=metadata,\n",
    "    schema_table_name=cfg.database.address_data.schema_table_name,\n",
    "    primary_key=address_data[\"_row_hash_id\"].to_list(),\n",
    ")\n",
    "\n",
    "# Insert data into database\n",
    "with engine.connect() as con:\n",
    "    address_data.to_sql(\n",
    "        name=cfg.database.address_data.table_name, schema=cfg.database.address_data.schema_name, con=con, if_exists=\"append\", index=False, chunksize=10000\n",
    "    )\n",
    "\n",
    "logger.info(f\"Loaded {address_data.shape[0]} rows of data into table {cfg.database.address_data.schema_table_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process town data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Reprocessing town data......\")\n",
    "\n",
    "# Read all data from a single file\n",
    "# NOTE it can currently only read from a single file\n",
    "data = pd.read_csv(cfg.town_data_path)\n",
    "\n",
    "# Calculate proportions for all numbers\n",
    "\n",
    "total_population = data[\"population_below24\"] + data[\"population_2559\"] + data[\"population_above60\"]\n",
    "data[\"pct_population_below24\"] = data[\"population_below24\"] / total_population\n",
    "data[\"pct_population_2559\"] = data[\"population_2559\"] / total_population\n",
    "data[\"pct_population_above60\"] = data[\"population_above60\"] / total_population\n",
    "\n",
    "total_property = data[\"num_hdb\"] + data[\"num_condo\"] + data[\"num_landed\"]\n",
    "data[\"pct_hdb\"] = data[\"num_hdb\"] / total_property\n",
    "data[\"pct_condo\"] = data[\"num_condo\"] / total_property\n",
    "data[\"pct_landed\"] = data[\"num_landed\"] / total_property\n",
    "\n",
    "total_income = data[\"income_below4k\"] + data[\"income_4k8k\"] + data[\"income_8k12k\"] + data[\"income_above12k\"]\n",
    "data[\"pct_income_below4k\"] = data[\"income_below4k\"] / total_income\n",
    "data[\"pct_income_4k8k\"] = data[\"income_4k8k\"] / total_income\n",
    "data[\"pct_income_8k12k\"] = data[\"income_8k12k\"] / total_income\n",
    "data[\"pct_income_above12k\"] = data[\"income_above12k\"] / total_income\n",
    "\n",
    "total_disability = data[\"household_wodisability\"] + data[\"household_withdisability\"]\n",
    "data[\"pct_household_wodisability\"] = data[\"household_wodisability\"] / total_disability\n",
    "data[\"pct_household_withdisability\"] = data[\"household_withdisability\"] / total_disability\n",
    "\n",
    "# Read geocoded data\n",
    "logger.info(\"Loading geocoded address data......\")\n",
    "add_query = sqlalchemy.text(\"SELECT * FROM hdb_resale.geolocation\")\n",
    "add_data = pd.read_sql(add_query, con=engine)\n",
    "\n",
    "# Get median lat & lng per town\n",
    "# Needed for predictions\n",
    "median_add = add_data.groupby(by=[\"town\"], as_index=False).agg({\"lat\": \"median\", \"lng\": \"median\"})\n",
    "\n",
    "data = data.merge(\n",
    "    median_add[[\"town\", \"lat\", \"lng\"]],\n",
    "    how=\"left\",\n",
    "    on=[\"town\"],\n",
    ")\n",
    "\n",
    "# # Write data to database\n",
    "# schema_name = \"hdb_resale\"\n",
    "# table_name = \"town\"\n",
    "# delete_table(schema=schema_name, table=table_name, con=engine)\n",
    "# data.to_sql(name=table_name, con=engine, schema=schema_name, index=False, if_exists=\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

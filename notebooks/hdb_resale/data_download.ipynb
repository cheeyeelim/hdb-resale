{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import hashlib\n",
    "import datetime\n",
    "from dateutil.rrule import rrule, MONTHLY\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from airflow_submodule.hdb_resale import api, sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve environment variables\n",
    "load_dotenv()\n",
    "\n",
    "POSTGRESQL_DASH_USER = os.environ.get(\"POSTGRESQL_DASH_USER\")\n",
    "POSTGRESQL_DASH_PASSWORD = os.environ.get(\"POSTGRESQL_DASH_PASSWORD\")\n",
    "POSTGRESQL_DASH_DATABASE = os.environ.get(\"POSTGRESQL_DASH_DATABASE\")\n",
    "POSTGRESQL_HOST = os.environ.get(\"POSTGRESQL_HOST\")\n",
    "POSTGRESQL_PORT = os.environ.get(\"POSTGRESQL_PORT\")\n",
    "\n",
    "# Setup logger\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set run parameters\n",
    "# Whether to do full run or delta run\n",
    "run_mode = \"full\" # \"delta\"\n",
    "# For delta run, how many past N months to cover from this month\n",
    "# If this month is Mar and past_n_mth = 3, then Jan-Mar will be covered\n",
    "past_n_mth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the connection string\n",
    "connection_string = f\"postgresql+psycopg2://{POSTGRESQL_DASH_USER}:{POSTGRESQL_DASH_PASSWORD}@{POSTGRESQL_HOST}:{POSTGRESQL_PORT}/{POSTGRESQL_DASH_DATABASE}\"\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Define schema & tables in database using SQLAlchemy\n",
    "metadata_obj = sql.define_all_schema_table()\n",
    "\n",
    "# Create schema & tables if not exist\n",
    "sql.create_all_table(engine=engine, metadata=metadata_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR TESTING - drop all tables\n",
    "# sql.drop_all_table(engine=engine, metadata=metadata_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_hdb_url = api.build_sggov_hdb_url()\n",
    "\n",
    "table_name = \"hdb_resale.source_data\"\n",
    "# NOTE it is unable to uniquely identify a flat without using _id provided, \n",
    "# as it is possible for many flats to share the exact same characteristics captured here\n",
    "hash_key_columns = [\n",
    "    \"_id\", \"month\", \"town\", \n",
    "    \"flat_type\", \"block\", \"street_name\", \n",
    "    \"storey_range\", \"floor_area_sqm\", \n",
    "    \"flat_model\", \"lease_commence_date\", \n",
    "    \"resale_price\"\n",
    "]\n",
    "\n",
    "end_month = datetime.date.today().replace(day=1) #datetime.date(2024, 3, 1)\n",
    "if run_mode==\"full\":\n",
    "    start_month = datetime.date(2017, 1, 1) #datetime.date(2024, 1, 1)\n",
    "elif run_mode==\"delta\":\n",
    "    start_month = end_month - relativedelta(months=past_n_mth-1)\n",
    "else:\n",
    "    raise Exception(f\"run_mode {run_mode} not implemented.\")\n",
    "range_month = [dt.date() for dt in rrule(MONTHLY, dtstart=start_month, until=end_month)] # create a range of months\n",
    "\n",
    "limit = 100 # following default of the API\n",
    "start_offset = 0 # starting offset value\n",
    "\n",
    "for cur_month in range_month: # loop through all months\n",
    "\n",
    "    cur_offset = start_offset\n",
    "    cur_row_retrieved = 0\n",
    "    exp_row_retrieved = 0\n",
    "    first_while_loop = True\n",
    "\n",
    "    while first_while_loop or not res.empty: # loop through all rows\n",
    "        # Get the next final formatted URL with base and query strings\n",
    "        data_query = f'{{\"month\":\"{cur_month.strftime(\"%Y-%m\")}\"}}'\n",
    "        api_final_url = f\"{api_hdb_url}&q={data_query}&limit={limit}&offset={cur_offset}\"\n",
    "\n",
    "        logger.info(f\"Retrieving data from endpoint with query - {api_final_url}\")\n",
    "\n",
    "        res = api.get_sggov_hdb_data(api_url=api_final_url)\n",
    "\n",
    "        # Break while loop when there is no longer any data\n",
    "        if res.empty:\n",
    "            assert int(cur_row_retrieved) == int(exp_row_retrieved) # Should have retrieved same number of rows as reported by API endpoint\n",
    "            break\n",
    "\n",
    "        # Rename original column names\n",
    "        res = res.rename(columns={\"rank month\":\"rank_month\"})\n",
    "\n",
    "        # Create row hash identifier using key columns\n",
    "        def _create_hash(row):\n",
    "            \"\"\"Convert all selected columns into strings, combine them into one and calculate hash.\"\"\"\n",
    "            row_id = '_'.join(row.values.astype(str)).encode(\"utf-8\")\n",
    "            row_id = hashlib.sha1(row_id).hexdigest()\n",
    "\n",
    "            return row_id\n",
    "\n",
    "        res[\"_row_hash_id\"] = res[hash_key_columns].apply(_create_hash, axis=1)\n",
    "\n",
    "        # Remove data based on primary key\n",
    "        # Needs to be done before data insertion to prevent database duplicated errors\n",
    "        # No data will be removed if the primary key does not exist\n",
    "        sql.delete_data_primary_key(\n",
    "            engine=engine, metadata=metadata_obj, \n",
    "            table_name=table_name, \n",
    "            primary_key=res[\"_row_hash_id\"].to_list()\n",
    "        )\n",
    "\n",
    "        # Insert data into database\n",
    "        with engine.connect() as con:\n",
    "            res.to_sql(name=\"source_data\", schema=\"hdb_resale\", con=con, if_exists=\"append\", index=False, chunksize=10000)\n",
    "\n",
    "        logger.info(f\"Loaded {res.shape[0]} rows of data into table {table_name}.\")\n",
    "\n",
    "        cur_offset += limit\n",
    "        cur_row_retrieved += res.shape[0]\n",
    "        exp_row_retrieved = res[\"_full_count\"][0] # This is the total row counts reported by API endpoint\n",
    "        first_while_loop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
